---
title: "p8105_hw5_hq2182"
author: "Hanfei Qi"
date: "11/13/2020"
output: github_document
---

```{r setup, include = FALSE, message = FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Read the data.
Create a city_state variable (e.g. “Baltimore, MD”).
Create a variable s.t. "Closed w/o arrest" & "No arrest" = "unsolved", while "Close by arrest" = "solved"  

Description of Raw data:  
There are 12 variables in the data on homicides in 50 large U.S. cities. Including a unique uid for each case, reported date, city, state, location information (latitude and longitude), and disposition of the case. The data also contains last name, first name, age, gender, and biological sex of victims.
```{r, message=FALSE}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa, AL")
```

Creat a new dataframe to store #total cases and #unsolved cases.
```{r, message=FALSE}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Run `prop.test` for "Baltimore, MD";
Apply `broom::tidy` to the result and pull the estimated proportion and confidence intervals,
then save them in another dataframe.
```{r}
Baltimore_prop = 
  prop.test(
  aggregate_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_unsolved),  
  aggregate_df %>% filter(city_state == "Baltimore, MD") %>% pull(hom_total)) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)

Baltimore_prop
```

Run `prop.test` for each city and extract estimated proportion and confidence intervals.
```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)

head(results_df)
```

Create a plot that shows the estimates and CIs for each city, add error bars based on the upper and lower limits.
```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  ylab("Unsolved Homicides Proportion") +
  xlab("City, State") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


# Problem 2

Read, clean, and tidy data.  
`arm` shows which group this row belongs to. "con" means control group and "exp" means experimental group.  
`id` contains identical id for each participant.
`week` is the number of week of observation.  
`observations` is the measured value of interest.  
```{r, message=FALSE}
data_df = 
  tibble(
    file_name = list.files("lda_data"),
  ) %>% 
  mutate(
    path = str_c("lda_data/", file_name),
    data = map(.x = path, ~read_csv(.x)),
    arm = str_extract(file_name, "[a-z]{1,3}"),
    id = readr::parse_number(file_name)) %>% 
  select(id, arm, data) %>% 
  unnest(data) %>% 
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observations") %>% 
  mutate(
    week = readr::parse_number(week)
  )
```
```{r}
head(data_df)
```


Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.
```{r}
data_df %>% 
  unite(arm_id, c(arm, id), sep = " ", remove = FALSE) %>% 
  select(-id) %>% 
  ggplot(aes(x = week, y = observations, color = arm_id)) +
  geom_point(aes(group = arm_id, color = arm)) + 
  geom_line(aes(group = arm_id, color = arm))  
```

Comment: It's obvious that observation values of experimental group are larger than observation values of control groups. There were overlapping values between two groups at the beginning of experiment. However, observations were distinct bewteen two groups at week 7 and 8 (the end of experiment).


## Problem 3

Generate dataset, perform `t.test`, and extract estimate mean and p-value.
For the `t.test`, the null hypothesis is true mean equals to 0, while alternative hypothesis is true mean is not equal to 0.  
The default setting is sample size = 30, mean = 0, and standard deviation = 5.  
The estimate mean and p-value of test will be saved in a tibble.  
`sim_result` run 5000 times for each true_mean from 0 to 6, then save the result in a tibble.  
```{r}
sim_est_p = function(n = 30, mu = 0, sigma = 5) {
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma))
  
  test = t.test(sim_data)
  
  tibble(
    estimate = test$estimate,
    p_value = test$p.value
  )
}

set.seed(5000)
sim_result = 
  tibble(true_mean = c(0:6)) %>% 
  mutate(
    output_lists = map(.x = true_mean, ~rerun(5000, sim_est_p(mu = .x))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df)
```


Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.
Given alpha = 0.05, so we reject the null when p<0.05.  
```{r}
prop_plot = 
  sim_result %>% 
  filter(p_value < 0.05) %>% 
  group_by(true_mean) %>% 
  mutate(
    rejected = n(),
    power = rejected/5000
  ) %>% 
  ggplot(aes(x = true_mean, y = power)) +
  geom_point()

prop_plot
```
Comment: As the true mean increases from 0 to 6, the power of the test increases (easier to reject the null). We almost reject the null for all estimated mean from dataset with true mean = 5 and 6. The power is close to 1.  

Make a plot showing the average estimate of mean (μ_hat) on the y axis and the true value of μ on the x axis.
Make a second plot the average μ_hat only in samples for which the null was rejected on the y axis and the true value of μ on the x axis.
```{r}
est_mu_plot = 
  sim_result %>% 
  group_by(true_mean) %>% 
  mutate(
    avg_estimate_mean = mean(estimate)
  ) %>% 
  ggplot(aes(x = true_mean, y = avg_estimate_mean)) +
  geom_point()

reject_plot = 
  sim_result %>% 
  filter(p_value < 0.05) %>% 
  group_by(true_mean) %>% 
  mutate(
    avg_estimate_mean = mean(estimate)
  ) %>% 
  ggplot(aes(x = true_mean, y = avg_estimate_mean)) +
  geom_point()

est_mu_plot / reject_plot
```

Comment:  

  *  For the first plot, the average μ_hat is very close to the true mean because we have a large sample size.  
  *  For the second plot, when the null is rejected, all average μ_hat of those dataare far away from 0. When the true mean is close to 0, the μ_hat has to be different from the true mean to reject the null. If the true mean >= 3, we will reject the null for μ_hat that closes to the true mean because it's considered different from mu = 0 already.

